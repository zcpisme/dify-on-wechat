# encoding:utf-8
import json
import os
import html
import re
from urllib.parse import urlparse
import time
import random
import asyncio
import nest_asyncio
import requests
from newspaper import Article
import newspaper
from bs4 import BeautifulSoup

# å¯¼å…¥requests_htmlç”¨äºåŠ¨æ€å†…å®¹æå–
from requests_html import HTMLSession

# åº”ç”¨nest_asyncioä»¥è§£å†³äº‹ä»¶å¾ªç¯é—®é¢˜
try:
    nest_asyncio.apply()
except Exception as e:
    logger.warning(f"[JinaSum] æ— æ³•åº”ç”¨nest_asyncio: {str(e)}")

import plugins
from bridge.context import ContextType
from bridge.reply import Reply, ReplyType
from common.log import logger
from plugins import *

@plugins.register(
    name="JinaSum",
    desire_priority=10,
    hidden=False,
    enabled=False,
    desc="Sum url link content with jina reader and llm",
    version="0.0.1",
    author="hanfangyuan",
)
class JinaSum(Plugin):

    jina_reader_base = "https://r.jina.ai"
    open_ai_api_base = "https://api.openai.com/v1"
    open_ai_model = "gpt-3.5-turbo"
    max_words = 8000
    prompt = "æˆ‘éœ€è¦å¯¹ä¸‹é¢å¼•å·å†…æ–‡æ¡£è¿›è¡Œæ€»ç»“ï¼Œæ€»ç»“è¾“å‡ºåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†ï¼š\nğŸ“– ä¸€å¥è¯æ€»ç»“\nğŸ”‘ å…³é”®è¦ç‚¹,ç”¨æ•°å­—åºå·åˆ—å‡º3-5ä¸ªæ–‡ç« çš„æ ¸å¿ƒå†…å®¹\nğŸ· æ ‡ç­¾: #xx #xx\nè¯·ä½¿ç”¨emojiè®©ä½ çš„è¡¨è¾¾æ›´ç”ŸåŠ¨\n\n"
    white_url_list = []
    black_url_list = [
        "https://support.weixin.qq.com", # è§†é¢‘å·è§†é¢‘
        "https://channels-aladin.wxqcloud.qq.com", # è§†é¢‘å·éŸ³ä¹
    ]

    def __init__(self):
        super().__init__()
        try:
            self.config = super().load_config()
            if not self.config:
                self.config = self._load_config_template()
            self.jina_reader_base = self.config.get("jina_reader_base", self.jina_reader_base)
            self.open_ai_api_base = self.config.get("open_ai_api_base", self.open_ai_api_base)
            self.open_ai_api_key = self.config.get("open_ai_api_key", "")
            self.open_ai_model = self.config.get("open_ai_model", self.open_ai_model)
            self.max_words = self.config.get("max_words", self.max_words)
            self.prompt = self.config.get("prompt", self.prompt)
            self.white_url_list = self.config.get("white_url_list", self.white_url_list)
            self.black_url_list = self.config.get("black_url_list", self.black_url_list)
            logger.info(f"[JinaSum] inited, config={self.config}")
            self.handlers[Event.ON_HANDLE_CONTEXT] = self.on_handle_context
        except Exception as e:
            logger.error(f"[JinaSum] åˆå§‹åŒ–å¼‚å¸¸ï¼š{e}")
            raise "[JinaSum] init failed, ignore "

    def on_handle_context(self, e_context: EventContext, retry_count: int = 0):
        try:
            context = e_context["context"]
            content = context.content
            if context.type != ContextType.SHARING and context.type != ContextType.TEXT:
                return
            if not self._check_url(content):
                logger.debug(f"[JinaSum] {content} is not a valid url, skip")
                return
            if retry_count == 0:
                logger.debug("[JinaSum] on_handle_context. content: %s" % content)
                reply = Reply(ReplyType.TEXT, "ğŸ‰æ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆæ€»ç»“ï¼Œè¯·ç¨å€™...")
                channel = e_context["channel"]
                channel.send(reply, context)

            target_url = html.unescape(content) # è§£å†³å…¬ä¼—å·å¡ç‰‡é“¾æ¥æ ¡éªŒé—®é¢˜ï¼Œå‚è€ƒ https://github.com/fatwang2/sum4all/commit/b983c49473fc55f13ba2c44e4d8b226db3517c45

            # å…ˆå°è¯•ä½¿ç”¨newspaper3kæå–å†…å®¹
            target_url_content = None
            
            # ä½¿ç”¨newspaper3k
            logger.debug("[JinaSum] å°è¯•ä½¿ç”¨newspaper3kæå–å†…å®¹")
            target_url_content = self._get_content_via_newspaper(target_url)
            
            # å¦‚æœnewspaper3kæå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é€šç”¨æ–¹æ³•
            if not target_url_content:
                logger.debug("[JinaSum] newspaper3kæå–å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é€šç”¨æ–¹æ³•")
                target_url_content = self._extract_content_general(target_url)
            
            # å¦‚æœå‰ä¸¤ç§æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨jinaæå–
            if not target_url_content:
                logger.debug("[JinaSum] æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨jinaæå–")
                target_url_content = self._extract_content_by_jina(target_url)
            
            if not target_url_content:
                logger.error("[JinaSum] æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œæ— æ³•æå–å†…å®¹")
                reply = Reply(ReplyType.ERROR, "æˆ‘æš‚æ—¶æ— æ³•æ€»ç»“é“¾æ¥ï¼Œè¯·ç¨åå†è¯•")
                e_context["reply"] = reply
                e_context.action = EventAction.BREAK_PASS
                return

            # æ¸…æ´—ç½‘é¡µå†…å®¹
            if target_url_content:
                target_url_content = self._clean_content(target_url_content)
            
            # è·å–APIå‚æ•°
            openai_chat_url = self._get_openai_chat_url()
            openai_headers = self._get_openai_headers()
            openai_payload = self._get_openai_payload(target_url_content)
            logger.debug(f"[JinaSum] openai_chat_url: {openai_chat_url}, openai_headers: {openai_headers}, openai_payload: {openai_payload}")
            
            # å‘é€è¯·æ±‚è·å–æ‘˜è¦
            response = requests.post(openai_chat_url, headers=openai_headers, json=openai_payload, timeout=60)
            response.raise_for_status()
            result = response.json()['choices'][0]['message']['content']
            
            # æ„å»ºå›å¤
            reply = Reply(ReplyType.TEXT, result)
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

        except Exception as e:
            if retry_count < 3:
                logger.warning(f"[JinaSum] {str(e)}, retry {retry_count + 1}")
                self.on_handle_context(e_context, retry_count + 1)
                return

            logger.exception(f"[JinaSum] {str(e)}")
            reply = Reply(ReplyType.ERROR, "æˆ‘æš‚æ—¶æ— æ³•æ€»ç»“é“¾æ¥ï¼Œè¯·ç¨åå†è¯•")
            e_context["reply"] = reply
            e_context.action = EventAction.BREAK_PASS

    def get_help_text(self, verbose, **kwargs):
        return f'ä½¿ç”¨å¤šç§ç½‘é¡µå†…å®¹æå–æ–¹å¼å’ŒChatGPTæ€»ç»“ç½‘é¡µé“¾æ¥å†…å®¹'

    def _load_config_template(self):
        logger.debug("No Suno plugin config.json, use plugins/jina_sum/config.json.template")
        try:
            plugin_config_path = os.path.join(self.path, "config.json.template")
            if os.path.exists(plugin_config_path):
                with open(plugin_config_path, "r", encoding="utf-8") as f:
                    plugin_conf = json.load(f)
                    return plugin_conf
        except Exception as e:
            logger.exception(e)

    def _get_jina_url(self, target_url):
        return self.jina_reader_base + "/" + target_url

    def _extract_content_by_jina(self, target_url):
        """ä½¿ç”¨Jina Readeræå–URLå†…å®¹
        
        Args:
            target_url: ç›®æ ‡URL
            
        Returns:
            str: æå–çš„å†…å®¹æ–‡æœ¬
            
        Raises:
            Exception: å½“è¯·æ±‚å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
        """
        try:
            logger.debug(f"[JinaSum] ä½¿ç”¨Jinaæå–å†…å®¹: {target_url}")
            jina_url = self._get_jina_url(target_url)
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36"}
            response = requests.get(jina_url, headers=headers, timeout=60)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"[JinaSum] Jinaæå–å¤±è´¥: {str(e)}")
            return None

    # ä»¥ä¸‹ç½‘é¡µå†…å®¹æå–åŠŸèƒ½å‚è€ƒäº† https://github.com/sofs2005/jina_sum
    def _get_content_via_newspaper(self, url):
        """ä½¿ç”¨newspaper3kåº“æå–æ–‡ç« å†…å®¹
        
        Args:
            url: æ–‡ç« URL
            
        Returns:
            str: æ–‡ç« å†…å®¹,å¤±è´¥è¿”å›None
        """
        try:
            # å¤„ç†Bç«™çŸ­é“¾æ¥
            if "b23.tv" in url:
                # å…ˆè·å–é‡å®šå‘åçš„çœŸå®URL
                try:
                    logger.debug(f"[JinaSum] è§£æBç«™çŸ­é“¾æ¥: {url}")
                    headers = {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
                    }
                    response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)
                    if response.status_code == 200:
                        real_url = response.url
                        logger.debug(f"[JinaSum] Bç«™çŸ­é“¾æ¥è§£æç»“æœ: {real_url}")
                        url = real_url
                except Exception as e:
                    logger.error(f"[JinaSum] è§£æBç«™çŸ­é“¾æ¥å¤±è´¥: {str(e)}")
            
            # é€‰æ‹©éšæœºUser-Agent
            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
            ]
            selected_ua = random.choice(user_agents)
            
            # æ„å»ºè¯·æ±‚å¤´
            headers = {
                "User-Agent": selected_ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
            }
            
            # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç‰¹æ®Šå¤„ç†
            if "mp.weixin.qq.com" in url:
                try:
                    # æ·»åŠ å¾®ä¿¡Cookieå‚æ•°
                    cookies = {
                        "appmsglist_action_" + str(int(time.time())): "card",
                        "pac_uid": f"{int(time.time())}_f{random.randint(10000, 99999)}",
                    }
                    
                    # ç›´æ¥è¯·æ±‚
                    session = requests.Session()
                    response = session.get(url, headers=headers, cookies=cookies, timeout=20)
                    response.raise_for_status()
                    
                    # ä½¿ç”¨BeautifulSoupè§£æ
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # æŸ¥æ‰¾å¾®ä¿¡æ–‡ç« å…ƒç´ 
                    title_elem = soup.select_one('#activity-name')
                    author_elem = soup.select_one('#js_name') or soup.select_one('#js_profile_qrcode > div > strong')
                    content_elem = soup.select_one('#js_content')
                    
                    if content_elem:
                        # ç§»é™¤æ— ç”¨å…ƒç´ 
                        for remove_elem in content_elem.select('script, style, svg'):
                            remove_elem.extract()
                            
                        # è·å–æ‰€æœ‰æ–‡æœ¬
                        text_content = content_elem.get_text(separator='\n', strip=True)
                        
                        if text_content and len(text_content) > 200:
                            title = title_elem.get_text(strip=True) if title_elem else ""
                            author = author_elem.get_text(strip=True) if author_elem else "æœªçŸ¥ä½œè€…"
                            
                            # æ„å»ºå†…å®¹
                            full_content = ""
                            if title:
                                full_content += f"æ ‡é¢˜: {title}\n"
                            if author and author != "æœªçŸ¥ä½œè€…":
                                full_content += f"ä½œè€…: {author}\n"
                            full_content += f"\n{text_content}"
                            
                            logger.debug(f"[JinaSum] æˆåŠŸæå–å¾®ä¿¡æ–‡ç« å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
                            return full_content
                except Exception as e:
                    logger.error(f"[JinaSum] ç›´æ¥æå–å¾®ä¿¡æ–‡ç« å¤±è´¥: {str(e)}")
            
            # é…ç½®newspaper
            newspaper.Config().browser_user_agent = selected_ua
            newspaper.Config().request_timeout = 30
            newspaper.Config().fetch_images = False
            
            # å°è¯•ä½¿ç”¨newspaperæå–
            try:
                # åˆ›å»ºArticleå¯¹è±¡
                article = Article(url, language='zh')
                
                # æ‰‹åŠ¨ä¸‹è½½
                session = requests.Session()
                response = session.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                
                # è®¾ç½®htmlå†…å®¹
                article.html = response.text
                article.download_state = 2  # ä¸‹è½½å®Œæˆ
                
                # è§£æ
                article.parse()
            except Exception as e:
                logger.error(f"[JinaSum] è‡ªå®šä¹‰ä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•: {str(e)}")
                article = Article(url, language='zh')
                article.download()
                article.parse()
            
            # è·å–å†…å®¹
            title = article.title
            authors = ', '.join(article.authors) if article.authors else "æœªçŸ¥ä½œè€…"
            publish_date = article.publish_date.strftime("%Y-%m-%d") if article.publish_date else "æœªçŸ¥æ—¥æœŸ"
            content = article.text
            
            # å¦‚æœå†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œå°è¯•ç›´æ¥ä»HTMLè·å–
            if not content or len(content) < 500:
                logger.debug("[JinaSum] Articleå†…å®¹å¤ªçŸ­ï¼Œç›´æ¥ä»HTMLæå–")
                try:
                    soup = BeautifulSoup(article.html, 'html.parser')
                    
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in soup(["script", "style"]):
                        script.extract()
                    
                    # è·å–æ‰€æœ‰æ–‡æœ¬
                    text = soup.get_text(separator='\n', strip=True)
                    
                    # å¦‚æœå†…å®¹æ›´é•¿ï¼Œä½¿ç”¨å®ƒ
                    if len(text) > len(content):
                        content = text
                        logger.debug(f"[JinaSum] ä½¿ç”¨BeautifulSoupæå–çš„å†…å®¹: {len(content)}å­—ç¬¦")
                except Exception as bs_error:
                    logger.error(f"[JinaSum] BeautifulSoupæå–å¤±è´¥: {str(bs_error)}")
            
            # åˆæˆæœ€ç»ˆå†…å®¹
            if title:
                full_content = f"æ ‡é¢˜: {title}\n"
                if authors and authors != "æœªçŸ¥ä½œè€…":
                    full_content += f"ä½œè€…: {authors}\n"
                if publish_date and publish_date != "æœªçŸ¥æ—¥æœŸ":
                    full_content += f"å‘å¸ƒæ—¥æœŸ: {publish_date}\n"
                full_content += f"\n{content}"
            else:
                full_content = content
            
            if not full_content or len(full_content.strip()) < 50:
                logger.debug("[JinaSum] newspaperæ²¡æœ‰æå–åˆ°å†…å®¹")
                return None
                
            logger.debug(f"[JinaSum] newspaperæˆåŠŸæå–å†…å®¹ï¼Œé•¿åº¦: {len(full_content)}")
            return full_content
            
        except Exception as e:
            logger.error(f"[JinaSum] newspaperæå–å†…å®¹å‡ºé”™: {str(e)}")
            return None

    def _extract_content_general(self, url):
        """é€šç”¨ç½‘é¡µå†…å®¹æå–æ–¹æ³•
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            from bs4 import BeautifulSoup
            
            # è·å–é»˜è®¤å¤´ä¿¡æ¯
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7"
            }
            
            # æ·»åŠ éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(0.5, 2))
            
            # åˆ›å»ºä¼šè¯
            session = requests.Session()
            
            # å‘é€è¯·æ±‚
            logger.debug(f"[JinaSum] é€šç”¨æ–¹æ³•è¯·æ±‚: {url}")
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # ç¡®ä¿ç¼–ç æ­£ç¡®
            if response.encoding == 'ISO-8859-1':
                response.encoding = response.apparent_encoding
                
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤æ— ç”¨å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe']):
                element.extract()
            
            # æŸ¥æ‰¾æ ‡é¢˜
            title = None
            title_candidates = [
                soup.select_one('h1'),
                soup.select_one('title'),
                soup.select_one('.title'),
                soup.select_one('.article-title'),
                soup.select_one('[class*="title" i]'),
            ]
            
            for candidate in title_candidates:
                if candidate and candidate.text.strip():
                    title = candidate.text.strip()
                    break
            
            # æŸ¥æ‰¾å†…å®¹
            content_candidates = []
            content_selectors = [
                'article', 'main', '.content', '.article', '.post-content',
                '[class*="content" i]', '[class*="article" i]',
                '#content', '#article', '.body'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_candidates.extend(elements)
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå¯»æ‰¾æ–‡æœ¬æœ€å¤šçš„div
            if not content_candidates:
                paragraphs = {}
                for elem in soup.find_all(['p', 'div']):
                    text = elem.get_text(strip=True)
                    if len(text) > 100:
                        paragraphs[elem] = len(text)
                
                if paragraphs:
                    max_elem = max(paragraphs.items(), key=lambda x: x[1])[0]
                    if max_elem.name == 'div':
                        content_candidates.append(max_elem)
                    else:
                        parent = max_elem.parent
                        if parent and len(parent.find_all('p')) > 3:
                            content_candidates.append(parent)
                        else:
                            content_candidates.append(max_elem)
            
            # è¯„åˆ†é€‰æ‹©æœ€ä½³å†…å®¹
            best_content = None
            max_score = 0
            
            for element in content_candidates:
                # è®¡ç®—æ–‡æœ¬é•¿åº¦
                text = element.get_text(strip=True)
                text_length = len(text)
                
                # è®¡ç®—æ–‡æœ¬å¯†åº¦
                html_length = len(str(element))
                text_density = text_length / html_length if html_length > 0 else 0
                
                # è®¡ç®—æ®µè½æ•°é‡
                paragraphs = element.find_all('p')
                paragraph_count = len(paragraphs)
                
                # è¯„åˆ†
                score = (
                    text_length * 1.0 +
                    text_density * 100 +
                    paragraph_count * 30
                )
                
                # å‡åˆ†ï¼šé“¾æ¥è¿‡å¤š
                links = element.find_all('a')
                link_text_ratio = sum(len(a.get_text(strip=True)) for a in links) / text_length if text_length > 0 else 0
                if link_text_ratio > 0.5:
                    score *= 0.5
                
                # æ›´æ–°æœ€ä½³å†…å®¹
                if score > max_score:
                    max_score = score
                    best_content = element
            
            # æå–å†…å®¹
            static_content_result = None
            if best_content:
                # ç§»é™¤å¹¿å‘Š
                for ad in best_content.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()
                
                # è·å–æ–‡æœ¬
                content_text = best_content.get_text(separator='\n', strip=True)
                
                # æ¸…ç†å¤šä½™ç©ºè¡Œ
                content_text = re.sub(r'\n{3,}', '\n\n', content_text)
                
                # æ„å»ºç»“æœ
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"
                
                result += content_text
                
                logger.debug(f"[JinaSum] é€šç”¨æ–¹æ³•æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(result)}")
                static_content_result = result
            
            # åˆ¤æ–­é™æ€æå–çš„å†…å®¹è´¨é‡
            content_is_good = False
            if static_content_result:
                # å†…å®¹é•¿åº¦æ£€æŸ¥
                if len(static_content_result) > 1000:
                    content_is_good = True
                # ç»“æ„æ£€æŸ¥ - è‡³å°‘åº”è¯¥æœ‰å¤šä¸ªæ®µè½
                elif static_content_result.count('\n\n') >= 3:
                    content_is_good = True
            
            # å¦‚æœé™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–
            if not content_is_good:
                logger.debug("[JinaSum] é™æ€æå–å†…å®¹è´¨é‡ä¸ä½³ï¼Œå°è¯•åŠ¨æ€æå–")
                dynamic_content = self._extract_dynamic_content(url, headers)
                if dynamic_content:
                    logger.debug(f"[JinaSum] åŠ¨æ€æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(dynamic_content)}")
                    return dynamic_content
            
            return static_content_result
                
        except Exception as e:
            logger.error(f"[JinaSum] é€šç”¨æå–æ–¹æ³•å¤±è´¥: {str(e)}")
            return None

    def _extract_dynamic_content(self, url, headers=None):
        """ä½¿ç”¨JavaScriptæ¸²æŸ“æå–åŠ¨æ€é¡µé¢å†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            headers: å¯é€‰çš„è¯·æ±‚å¤´
            
        Returns:
            str: æå–çš„å†…å®¹ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.debug(f"[JinaSum] å¼€å§‹åŠ¨æ€æå–å†…å®¹: {url}")
            
            # åˆ›å»ºä¼šè¯å¹¶è®¾ç½®è¶…æ—¶
            session = HTMLSession()
            
            # å¦‚æœæ²¡æœ‰æä¾›headersï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not headers:
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                    "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7"
                }
            
            # è·å–é¡µé¢
            response = session.get(url, headers=headers, timeout=30)
            
            # æ‰§è¡ŒJavaScript (è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…)
            logger.debug("[JinaSum] å¼€å§‹æ‰§è¡ŒJavaScript")
            response.html.render(timeout=20, sleep=2)
            logger.debug("[JinaSum] JavaScriptæ‰§è¡Œå®Œæˆ")
            
            # å¤„ç†æ¸²æŸ“åçš„HTML
            rendered_html = response.html.html
            
            # ä½¿ç”¨BeautifulSoupè§£ææ¸²æŸ“åçš„HTML
            soup = BeautifulSoup(rendered_html, 'html.parser')
            
            # æ¸…ç†æ— ç”¨å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.extract()
            
            # æŸ¥æ‰¾æ ‡é¢˜
            title = None
            title_candidates = [
                soup.select_one('h1'),
                soup.select_one('title'),
                soup.select_one('.title'),
                soup.select_one('[class*="title" i]'),
            ]
            
            for candidate in title_candidates:
                if candidate and candidate.text.strip():
                    title = candidate.text.strip()
                    break
            
            # å¯»æ‰¾ä¸»è¦å†…å®¹
            main_content = None
            
            # 1. å°è¯•æ‰¾ä¸»è¦å†…å®¹å®¹å™¨
            main_selectors = [
                'article', 'main', '.content', '.article',
                '[class*="content" i]', '[class*="article" i]',
                '#content', '#article'
            ]
            
            for selector in main_selectors:
                elements = soup.select(selector)
                if elements:
                    # é€‰æ‹©åŒ…å«æœ€å¤šæ–‡æœ¬çš„å…ƒç´ 
                    main_content = max(elements, key=lambda x: len(x.get_text()))
                    break
            
            # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå¯»æ‰¾æ–‡æœ¬æœ€å¤šçš„div
            if not main_content:
                paragraphs = {}
                for elem in soup.find_all(['div']):
                    text = elem.get_text(strip=True)
                    if len(text) > 200:  # åªè€ƒè™‘é•¿æ–‡æœ¬
                        paragraphs[elem] = len(text)
                
                if paragraphs:
                    main_content = max(paragraphs.items(), key=lambda x: x[1])[0]
            
            # 3. å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´ä¸ªbody
            if not main_content:
                main_content = soup.body
            
            # ä»ä¸»è¦å†…å®¹ä¸­æå–æ–‡æœ¬
            if main_content:
                # æ¸…ç†å¯èƒ½çš„å¹¿å‘Šæˆ–æ— å…³å…ƒç´ 
                for ad in main_content.select('[class*="ad" i], [class*="banner" i], [id*="ad" i], [class*="recommend" i]'):
                    ad.extract()
                
                # è·å–æ–‡æœ¬
                content_text = main_content.get_text(separator='\n', strip=True)
                content_text = re.sub(r'\n{3,}', '\n\n', content_text)  # æ¸…ç†å¤šä½™ç©ºè¡Œ
                
                # æ„å»ºæœ€ç»ˆç»“æœ
                result = ""
                if title:
                    result += f"æ ‡é¢˜: {title}\n\n"
                result += content_text
                
                # å…³é—­ä¼šè¯
                session.close()
                
                return result
            
            # å…³é—­ä¼šè¯
            session.close()
            
            return None
            
        except Exception as e:
            logger.error(f"[JinaSum] åŠ¨æ€æå–å¤±è´¥: {str(e)}")
            return None

    def _clean_content(self, content: str) -> str:
        """æ¸…æ´—å†…å®¹ï¼Œå»é™¤æ— ç”¨ä¿¡æ¯
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            str: æ¸…æ´—åçš„å†…å®¹
        """
        if not content:
            return content
            
        # è®°å½•åŸå§‹é•¿åº¦
        original_length = len(content)
        
        # ç§»é™¤Markdownå›¾ç‰‡æ ‡ç­¾
        content = re.sub(r'!\[.*?\]\(.*?\)', '', content)
        content = re.sub(r'\[!\[.*?\]\(.*?\)', '', content)
        
        # ç§»é™¤å›¾ç‰‡æè¿°
        content = re.sub(r'\[å›¾ç‰‡\]|\[image\]|\[img\]|\[picture\]', '', content, flags=re.IGNORECASE)
        content = re.sub(r'\[.*?å›¾ç‰‡.*?\]', '', content)
        
        # ç§»é™¤å…ƒæ•°æ®
        content = re.sub(r'æœ¬æ–‡å­—æ•°ï¼š\d+ï¼Œé˜…è¯»æ—¶é•¿å¤§çº¦\d+åˆ†é’Ÿ', '', content)
        content = re.sub(r'é˜…è¯»æ—¶é•¿[:ï¼š].*?åˆ†é’Ÿ', '', content)
        content = re.sub(r'å­—æ•°[:ï¼š]\d+', '', content)
        
        # ç§»é™¤æ—¥æœŸå’Œæ—¶é—´æˆ³
        content = re.sub(r'\d{4}[\.å¹´/-]\d{1,2}[\.æœˆ/-]\d{1,2}[æ—¥å·]?(\s+\d{1,2}:\d{1,2}(:\d{1,2})?)?', '', content)
        
        # ç§»é™¤åˆ†éš”çº¿
        content = re.sub(r'\*\s*\*\s*\*', '', content)
        content = re.sub(r'-{3,}', '', content)
        content = re.sub(r'_{3,}', '', content)
        
        # ç§»é™¤å¹¿å‘Šæ ‡è®°
        ad_patterns = [
            r'å¹¿å‘Š\s*[\.ã€‚]?', 
            r'èµåŠ©å†…å®¹', 
            r'sponsored content',
            r'advertisement',
            r'æ¨å¹¿ä¿¡æ¯',
            r'\[å¹¿å‘Š\]',
            r'ã€å¹¿å‘Šã€‘',
        ]
        for pattern in ad_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        # ç§»é™¤URLé“¾æ¥
        content = re.sub(r'https?://\S+', '', content)
        content = re.sub(r'www\.\S+', '', content)
        
        # æ¸…ç†Markdownæ ¼å¼
        content = re.sub(r'\*\*(.+?)\*\*', r'\1', content)
        content = re.sub(r'\*(.+?)\*', r'\1', content)
        content = re.sub(r'`(.+?)`', r'\1', content)
        
        # æ¸…ç†æ–‡ç« å°¾éƒ¨
        content = re.sub(r'\*\*å¾®ä¿¡ç¼–è¾‘\*\*.*?$', '', content, flags=re.MULTILINE)
        content = re.sub(r'\*\*æ¨èé˜…è¯»\*\*.*?$', '', content, flags=re.MULTILINE | re.DOTALL)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r'\s{2,}', ' ', content)
        content = re.sub(r'^\s+', '', content, flags=re.MULTILINE)
        content = re.sub(r'\s+$', '', content, flags=re.MULTILINE)
        
        # è®°å½•æ¸…æ´—åé•¿åº¦
        cleaned_length = len(content)
        logger.debug(f"[JinaSum] å†…å®¹æ¸…æ´—: åŸå§‹é•¿åº¦={original_length}, æ¸…æ´—åé•¿åº¦={cleaned_length}, å‡å°‘={original_length - cleaned_length}")
        
        return content

    def _get_openai_chat_url(self):
        return self.open_ai_api_base + "/chat/completions"

    def _get_openai_headers(self):
        return {
            'Authorization': f"Bearer {self.open_ai_api_key}",
            'Host': urlparse(self.open_ai_api_base).netloc
        }

    def _get_openai_payload(self, target_url_content):
        target_url_content = target_url_content[:self.max_words] # é€šè¿‡å­—ç¬¦ä¸²é•¿åº¦ç®€å•è¿›è¡Œæˆªæ–­
        sum_prompt = f"{self.prompt}\n\n'''{target_url_content}'''"
        messages = [{"role": "user", "content": sum_prompt}]
        payload = {
            'model': self.open_ai_model,
            'messages': messages
        }
        return payload

    def _check_url(self, target_url: str):
        stripped_url = target_url.strip()
        # ç®€å•æ ¡éªŒæ˜¯å¦æ˜¯url
        if not stripped_url.startswith("http://") and not stripped_url.startswith("https://"):
            return False

        # æ£€æŸ¥ç™½åå•
        if len(self.white_url_list):
            if not any(stripped_url.startswith(white_url) for white_url in self.white_url_list):
                return False

        # æ’é™¤é»‘åå•ï¼Œé»‘åå•ä¼˜å…ˆçº§>ç™½åå•
        for black_url in self.black_url_list:
            if stripped_url.startswith(black_url):
                return False

        return True
